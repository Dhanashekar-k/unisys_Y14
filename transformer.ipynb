{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3391,"status":"ok","timestamp":1714135885756,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"EtjzvqyLhBpZ","outputId":"4c3ff123-31c8-418b-a225-6938b27acaf7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391,"status":"ok","timestamp":1714135890804,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"frMp44LKhdrQ","outputId":"7c74c857-5907-4d84-900f-1f618c8e883c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/video_vision_transformer\n"]}],"source":["cd /content/drive/MyDrive/video_vision_transformer"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":715,"status":"ok","timestamp":1714135894348,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"IIwIF-4DhWF1"},"outputs":[],"source":["import os\n","import io\n","import imageio\n","import ipywidgets\n","import numpy as np\n","import tensorflow as tf  # for data preprocessing only\n","import keras\n","from keras import layers"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714135896965,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"ARekMdWshyhP"},"outputs":[],"source":["SEED = 42\n","os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n","keras.utils.set_random_seed(SEED)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":66228,"status":"ok","timestamp":1714135965129,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"8_vbxDnOQ4at"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","\n","def prepare_dataset(data_dir: str):\n","    train_data_dir = os.path.join(data_dir, \"TRAIN\")\n","    val_data_dir = os.path.join(data_dir, \"VAL\")\n","\n","    train_videos, train_labels = load_data(train_data_dir)\n","    valid_videos, valid_labels = load_data(val_data_dir)\n","\n","    return (\n","        (train_videos, train_labels),\n","        (valid_videos, valid_labels),\n","        None  # No test dataset provided\n","    )\n","\n","def load_data(data_dir):\n","    video_paths = []\n","    labels = []\n","    for phase_folder in os.listdir(data_dir):\n","        phase_folder_path = os.path.join(data_dir, phase_folder)\n","        if os.path.isdir(phase_folder_path):\n","            label = int(phase_folder)  # Assuming each phase folder is named with the phase number\n","            for file in os.listdir(phase_folder_path):\n","                if file.endswith(\".avi\"):\n","                    video_paths.append(os.path.join(phase_folder_path, file))\n","                    labels.append(label)\n","\n","    videos = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frames.append(frame)\n","        cap.release()\n","        videos.append(frames)\n","\n","    videos = np.array(videos)\n","    labels = np.array(labels)\n","\n","    # Convert labels to one-hot encoding\n","    num_classes = len(np.unique(labels))  # Get the number of unique labels\n","    labels = keras.utils.to_categorical(labels, num_classes=num_classes)\n","\n","    return videos, labels\n","\n","data_dir = \"/content/drive/MyDrive/video_vision_transformer/DEMO\"\n","(train_data, train_labels), (valid_data, valid_labels), _ = prepare_dataset(data_dir)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714137091144,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"vDutp6EEiSUD"},"outputs":[],"source":["import tensorflow as tf  # for data preprocessing only\n","#DATA\n","#DATASET_NAME = \"organmnist3d\"\n","BATCH_SIZE = 32\n","AUTO = tf.data.AUTOTUNE\n","INPUT_SHAPE = (224, 224, 25, 3)\n","NUM_CLASSES = 7\n","\n","# OPTIMIZER\n","LEARNING_RATE = 1e-4\n","WEIGHT_DECAY = 1e-5\n","\n","# TRAINING\n","EPOCHS = 60\n","\n","# TUBELET EMBEDDING\n","PATCH_SIZE = (224, 224, 25)\n","NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n","\n","# MedViViT ARCHITECTURE\n","LAYER_NORM_EPS = 1e-6\n","PROJECTION_DIM = 128\n","NUM_HEADS = 8\n","NUM_LAYERS = 8"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714137127831,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"muwB9HzLSBe-","outputId":"d0536a97-fa80-480a-d666-573f2236bfe2"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'import os\\nimport cv2\\n\\ndef get_video_metadata(data_dir):\\n    video_paths = []\\n    for root, dirs, files in os.walk(data_dir):\\n        for file in files:\\n            if file.endswith(\".avi\"):\\n                video_paths.append(os.path.join(root, file))\\n\\n    metadata = []\\n    for video_path in video_paths:\\n        cap = cv2.VideoCapture(video_path)\\n\\n        # Get video properties\\n        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\\n        fps = cap.get(cv2.CAP_PROP_FPS)\\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\\n        duration = num_frames / fps\\n\\n        # Append metadata\\n        metadata.append({\\n            \\'path\\': video_path,\\n            \\'num_frames\\': num_frames,\\n            \\'fps\\': fps,\\n            \\'resolution\\': (width, height),\\n            \\'duration_sec\\': duration\\n        })\\n\\n        cap.release()\\n\\n    return metadata\\n\\ndata_dir = \"/content/drive/MyDrive/video_vision_transformer/DEMO\"\\nmetadata = get_video_metadata(data_dir)\\n\\n# Print metadata of the first video\\nif metadata:\\n    meta = metadata[0]\\n    print(f\"Path: {meta[\\'path\\']}\")\\n    print(f\"Number of frames: {meta[\\'num_frames\\']}\")\\n    print(f\"FPS: {meta[\\'fps\\']}\")\\n    print(f\"Resolution: {meta[\\'resolution\\']}\")\\n    print(f\"Duration (seconds): {meta[\\'duration_sec\\']}\")\\nelse:\\n    print(\"No video metadata found.\")'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["'''import os\n","import cv2\n","\n","def get_video_metadata(data_dir):\n","    video_paths = []\n","    for root, dirs, files in os.walk(data_dir):\n","        for file in files:\n","            if file.endswith(\".avi\"):\n","                video_paths.append(os.path.join(root, file))\n","\n","    metadata = []\n","    for video_path in video_paths:\n","        cap = cv2.VideoCapture(video_path)\n","\n","        # Get video properties\n","        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        duration = num_frames / fps\n","\n","        # Append metadata\n","        metadata.append({\n","            'path': video_path,\n","            'num_frames': num_frames,\n","            'fps': fps,\n","            'resolution': (width, height),\n","            'duration_sec': duration\n","        })\n","\n","        cap.release()\n","\n","    return metadata\n","\n","data_dir = \"/content/drive/MyDrive/video_vision_transformer/DEMO\"\n","metadata = get_video_metadata(data_dir)\n","\n","# Print metadata of the first video\n","if metadata:\n","    meta = metadata[0]\n","    print(f\"Path: {meta['path']}\")\n","    print(f\"Number of frames: {meta['num_frames']}\")\n","    print(f\"FPS: {meta['fps']}\")\n","    print(f\"Resolution: {meta['resolution']}\")\n","    print(f\"Duration (seconds): {meta['duration_sec']}\")\n","else:\n","    print(\"No video metadata found.\")'''"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1714137785474,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"qekmr81diayQ"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","def preprocess(frames: tf.Tensor, label: tf.Tensor):\n","    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n","    # Resize frames to the desired shape (224x224)\n","    frames = tf.image.resize(frames, (224, 224))\n","    # Convert frames to float32 and normalize pixel values\n","    frames = tf.cast(frames, tf.float32) / 255.0\n","    # Transpose frames to match the expected input shape (None, 224, 224, 25, 3)\n","    frames = tf.transpose(frames, perm=[1, 2, 0, 3])  # Assuming the depth dimension is the last dimension\n","    # Parse label\n","    label = tf.cast(label, tf.float32)\n","    return frames, label\n","\n","def prepare_dataloader(\n","    videos: np.ndarray,\n","    labels: np.ndarray,\n","    loader_type: str = \"train\",\n","    batch_size: int = 32,\n","):\n","    \"\"\"Utility function to prepare the dataloader.\"\"\"\n","    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n","\n","    if loader_type == \"train\":\n","        dataset = dataset.shuffle(len(videos))  # Shuffle only for training\n","\n","    # Apply preprocessing and configure batching and prefetching\n","    dataloader = (\n","        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n","        .batch(batch_size)\n","        .prefetch(tf.data.AUTOTUNE)\n","    )\n","    return dataloader"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25914,"status":"ok","timestamp":1714137815955,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"iSwMw2Snid65","outputId":"4c0bb157-c386-4025-8680-1274e56e0436"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch of frames shape: (32, 224, 224, 25, 3)\n","Batch of labels shape: (32, 7)\n"]}],"source":["#expected_shape=(None, 224, 224, 25, 3)\n","\n","train_dataloader = prepare_dataloader(train_data, train_labels, loader_type=\"train\", batch_size=32)\n","valid_dataloader = prepare_dataloader(valid_data, valid_labels, loader_type=\"valid\", batch_size=32)\n","\n","# Iterate over batches in the dataloaders\n","for batch in train_dataloader.take(1):\n","    frames, labels = batch\n","    print(\"Batch of frames shape:\", frames.shape)\n","    print(\"Batch of labels shape:\", labels.shape)"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":583,"status":"ok","timestamp":1714138126712,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"QUBvtrT-igEL"},"outputs":[],"source":["class TubeletEmbedding(layers.Layer):\n","    def __init__(self, embed_dim, patch_size, **kwargs):\n","        super().__init__(**kwargs)\n","        self.projection = layers.Conv3D(\n","            filters=embed_dim,\n","            kernel_size=patch_size,\n","            strides=patch_size,\n","            padding=\"VALID\",\n","        )\n","        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n","\n","    def call(self, videos):\n","        projected_patches = self.projection(videos)\n","        flattened_patches = self.flatten(projected_patches)\n","        return flattened_patches"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714138127218,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"aOuSfmz0ikcy"},"outputs":[],"source":["class PositionalEncoder(layers.Layer):\n","    def __init__(self, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","\n","    def build(self, input_shape):\n","        _, num_tokens, _ = input_shape\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_tokens, output_dim=self.embed_dim\n","        )\n","        self.positions = tf.range(0, num_tokens, 1)\n","\n","    def call(self, encoded_tokens):\n","        # Encode the positions and add it to the encoded tokens\n","        encoded_positions = self.position_embedding(self.positions)\n","        encoded_tokens = encoded_tokens + encoded_positions\n","        return encoded_tokens"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714138127218,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"vlWU2wTEiq5C"},"outputs":[],"source":["def create_Medvivit_classifier(\n","    tubelet_embedder,\n","    positional_encoder,\n","    input_shape=INPUT_SHAPE,\n","    transformer_layers=NUM_LAYERS,\n","    num_heads=NUM_HEADS,\n","    embed_dim=PROJECTION_DIM,\n","    layer_norm_eps=LAYER_NORM_EPS,\n","    num_classes=NUM_CLASSES,\n","):\n","    # Get the input layer\n","    inputs = layers.Input(shape=input_shape)\n","    # Create patches.\n","    patches = tubelet_embedder(inputs)\n","    # Encode patches.\n","    encoded_patches = positional_encoder(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization and MHSA\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n","        )(x1, x1)\n","\n","        # Skip connection\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","\n","        # Layer Normalization and MLP\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        x3 = keras.Sequential(\n","            [\n","                layers.Dense(units=embed_dim * 4, activation='gelu'),\n","                layers.Dense(units=embed_dim, activation='gelu'),\n","            ]\n","        )(x3)\n","\n","        # Skip connection\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Layer normalization and Global average pooling.\n","    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n","    representation = layers.GlobalAvgPool1D()(representation)\n","\n","    # Classify outputs.\n","    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n","\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"6itPFgnfiz1X"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","22/22 [==============================] - 346s 15s/step - loss: 2.1739 - accuracy: 0.1671 - top-5-accuracy: 0.6786 - val_loss: 2.0385 - val_accuracy: 0.1429 - val_top-5-accuracy: 0.7041\n","Epoch 2/10\n"," 3/22 [===\u003e..........................] - ETA: 4:41 - loss: 1.9955 - accuracy: 0.2083 - top-5-accuracy: 0.6875"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-61-b136202d2f3f\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 33\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Assuming train_dataloader and valid_dataloader are properly initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 33\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-61-b136202d2f3f\u003e\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(train_dataloader, valid_dataloader, EPOCHS)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Train the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Evaluate on validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def run_experiment(train_dataloader, valid_dataloader, EPOCHS=10):\n","    # Initialize model\n","    model = create_Medvivit_classifier(\n","        tubelet_embedder=TubeletEmbedding(\n","            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n","        ),\n","        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n","    )\n","\n","    # Compile the model with the optimizer, loss function\n","    # and the metrics.\n","    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\n","            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n","            keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","\n","    # Train the model.\n","    model.fit(train_dataloader, epochs=EPOCHS, validation_data=valid_dataloader)\n","\n","    # Evaluate on validation data\n","    loss, accuracy = model.evaluate(valid_dataloader)\n","    print(f\"Validation loss: {loss:.4f}\")\n","    print(f\"Validation accuracy: {accuracy:.4f}\")\n","\n","    return model\n","\n","# Assuming train_dataloader and valid_dataloader are properly initialized\n","model = run_experiment(train_dataloader, valid_dataloader, EPOCHS=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1714138134803,"user":{"displayName":"Shekar K","userId":"08349865897291882999"},"user_tz":-330},"id":"HVZX8Wqyi23g"},"outputs":[],"source":["NUM_SAMPLES_VIZ = 25\n","testsamples, labels = next(iter(testloader))\n","testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n","\n","ground_truths = []\n","preds = []\n","videos = []\n","\n","for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n","    # Generate gif\n","    testsample = np.reshape(testsample.numpy(), (-1, 28, 28))\n","    with io.BytesIO() as gif:\n","        imageio.mimsave(gif, (testsample * 255).astype(\"uint8\"), \"GIF\", fps=5)\n","        videos.append(gif.getvalue())\n","\n","    # Get model prediction\n","    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n","\n","    pred = np.argmax(output, axis=0)\n","\n","    ground_truths.append(label.numpy().astype(\"int\"))\n","    preds.append(pred)\n","\n","\n","def make_box_for_grid(image_widget, fit):\n","    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n","\n","    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n","    \"\"\"\n","    # Make the caption\n","    if fit is not None:\n","        fit_str = \"'{}'\".format(fit)\n","    else:\n","        fit_str = str(fit)\n","\n","    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n","\n","    # Make the green box with the image widget inside it\n","    boxb = ipywidgets.widgets.Box()\n","    boxb.children = [image_widget]\n","\n","    # Compose into a vertical box\n","    vb = ipywidgets.widgets.VBox()\n","    vb.layout.align_items = \"center\"\n","    vb.children = [h, boxb]\n","    return vb\n","\n","\n","boxes = []\n","for i in range(NUM_SAMPLES_VIZ):\n","    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n","    true_class = info[\"label\"][str(ground_truths[i])]\n","    pred_class = info[\"label\"][str(preds[i])]\n","    caption = f\"T: {true_class} | P: {pred_class}\"\n","\n","    boxes.append(make_box_for_grid(ib, caption))\n","\n","ipywidgets.widgets.GridBox(\n","    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKF6bFeBkZ84"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyMK2U7ae3Xra9cpJUId9s2H","gpuType":"V28","mount_file_id":"1-9xx-QmqMxudvof5qFegKqarno4dRQDG","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}